2025-08-19 18:01:46 [twisted] CRITICAL: Unhandled error in Deferred:
2025-08-19 18:01:46 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 74, in load_object
    obj = getattr(mod, name)
AttributeError: module 'books.pipelines' has no attribute 'BooksPipeline'. Did you mean: 'MongoPipeline'?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\engine.py", line 114, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\scraper.py", line 107, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'books.pipelines' doesn't define any object named 'BooksPipeline'
2025-08-19 18:05:24 [twisted] CRITICAL: Unhandled error in Deferred:
2025-08-19 18:05:24 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 74, in load_object
    obj = getattr(mod, name)
AttributeError: module 'books.pipelines' has no attribute 'BooksPipeline'. Did you mean: 'MongoPipeline'?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\engine.py", line 114, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\scraper.py", line 107, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'books.pipelines' doesn't define any object named 'BooksPipeline'
2025-08-19 18:07:32 [twisted] CRITICAL: Unhandled error in Deferred:
2025-08-19 18:07:32 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 74, in load_object
    obj = getattr(mod, name)
AttributeError: module 'books.pipelines' has no attribute 'BooksPipeline'. Did you mean: 'MongoPipeline'?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\engine.py", line 114, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\scraper.py", line 107, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'books.pipelines' doesn't define any object named 'BooksPipeline'
2025-08-19 18:08:35 [twisted] CRITICAL: Unhandled error in Deferred:
2025-08-19 18:08:35 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 74, in load_object
    obj = getattr(mod, name)
AttributeError: module 'books.pipelines' has no attribute 'BooksPipeline'. Did you mean: 'MongoPipeline'?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\engine.py", line 114, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\scraper.py", line 107, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'books.pipelines' doesn't define any object named 'BooksPipeline'
2025-08-19 18:10:32 [twisted] CRITICAL: Unhandled error in Deferred:
2025-08-19 18:10:32 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 74, in load_object
    obj = getattr(mod, name)
AttributeError: module 'books.pipelines' has no attribute 'BooksPipeline'. Did you mean: 'MongoPipeline'?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\engine.py", line 114, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\scraper.py", line 107, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'books.pipelines' doesn't define any object named 'BooksPipeline'
2025-08-19 18:12:11 [twisted] CRITICAL: Unhandled error in Deferred:
2025-08-19 18:12:11 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 74, in load_object
    obj = getattr(mod, name)
AttributeError: module 'books.pipelines' has no attribute 'BooksPipeline'. Did you mean: 'MongoPipeline'?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\engine.py", line 114, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\scraper.py", line 107, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'books.pipelines' doesn't define any object named 'BooksPipeline'
2025-08-19 18:16:38 [twisted] CRITICAL: Unhandled error in Deferred:
2025-08-19 18:16:38 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 74, in load_object
    obj = getattr(mod, name)
AttributeError: module 'books.pipelines' has no attribute 'BooksPipeline'. Did you mean: 'MongoPipeline'?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\engine.py", line 114, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\scraper.py", line 107, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'books.pipelines' doesn't define any object named 'BooksPipeline'
2025-08-19 18:39:56 [py.warnings] WARNING: C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: books.spiders.book.BookSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-08-19 18:41:31 [py.warnings] WARNING: C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: books.spiders.book.BookSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-08-19 18:43:53 [py.warnings] WARNING: C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: books.spiders.book.BookSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-08-19 18:45:23 [py.warnings] WARNING: C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: books.spiders.book.BookSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-08-19 18:50:25 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: books)
2025-08-19 18:50:25 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.0',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.13.7 (tags/v3.13.7:bcee1c3, Aug 14 2025, 14:15:11) [MSC v.1944 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.6',
 'Platform': 'Windows-11-10.0.26100-SP0'}
2025-08-19 18:50:25 [scrapy.addons] INFO: Enabled addons:
[]
2025-08-19 18:50:25 [asyncio] DEBUG: Using selector: SelectSelector
2025-08-19 18:50:25 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-08-19 18:50:25 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-08-19 18:50:25 [scrapy.extensions.telnet] INFO: Telnet Password: 358a3b9314c4125b
2025-08-19 18:50:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2025-08-19 18:50:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'books',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
 'DOWNLOAD_DELAY': 1,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'book_scraper.log',
 'NEWSPIDER_MODULE': 'books.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['books.spiders']}
2025-08-19 18:50:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-08-19 18:50:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-08-19 18:50:26 [twisted] CRITICAL: Unhandled error in Deferred:
2025-08-19 18:50:26 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 74, in load_object
    obj = getattr(mod, name)
AttributeError: module 'books.pipelines' has no attribute 'BookPipeline'. Did you mean: 'MongoPipeline'?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\engine.py", line 114, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\scraper.py", line 107, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'books.pipelines' doesn't define any object named 'BookPipeline'
