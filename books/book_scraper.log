2025-08-19 18:01:46 [twisted] CRITICAL: Unhandled error in Deferred:
2025-08-19 18:01:46 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 74, in load_object
    obj = getattr(mod, name)
AttributeError: module 'books.pipelines' has no attribute 'BooksPipeline'. Did you mean: 'MongoPipeline'?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\engine.py", line 114, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\scraper.py", line 107, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'books.pipelines' doesn't define any object named 'BooksPipeline'
2025-08-19 18:05:24 [twisted] CRITICAL: Unhandled error in Deferred:
2025-08-19 18:05:24 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 74, in load_object
    obj = getattr(mod, name)
AttributeError: module 'books.pipelines' has no attribute 'BooksPipeline'. Did you mean: 'MongoPipeline'?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\engine.py", line 114, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\scraper.py", line 107, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'books.pipelines' doesn't define any object named 'BooksPipeline'
2025-08-19 18:07:32 [twisted] CRITICAL: Unhandled error in Deferred:
2025-08-19 18:07:32 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 74, in load_object
    obj = getattr(mod, name)
AttributeError: module 'books.pipelines' has no attribute 'BooksPipeline'. Did you mean: 'MongoPipeline'?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\engine.py", line 114, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\scraper.py", line 107, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'books.pipelines' doesn't define any object named 'BooksPipeline'
2025-08-19 18:08:35 [twisted] CRITICAL: Unhandled error in Deferred:
2025-08-19 18:08:35 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 74, in load_object
    obj = getattr(mod, name)
AttributeError: module 'books.pipelines' has no attribute 'BooksPipeline'. Did you mean: 'MongoPipeline'?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\engine.py", line 114, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\scraper.py", line 107, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'books.pipelines' doesn't define any object named 'BooksPipeline'
2025-08-19 18:10:32 [twisted] CRITICAL: Unhandled error in Deferred:
2025-08-19 18:10:32 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 74, in load_object
    obj = getattr(mod, name)
AttributeError: module 'books.pipelines' has no attribute 'BooksPipeline'. Did you mean: 'MongoPipeline'?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\engine.py", line 114, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\scraper.py", line 107, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'books.pipelines' doesn't define any object named 'BooksPipeline'
2025-08-19 18:12:11 [twisted] CRITICAL: Unhandled error in Deferred:
2025-08-19 18:12:11 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 74, in load_object
    obj = getattr(mod, name)
AttributeError: module 'books.pipelines' has no attribute 'BooksPipeline'. Did you mean: 'MongoPipeline'?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\engine.py", line 114, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\scraper.py", line 107, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'books.pipelines' doesn't define any object named 'BooksPipeline'
2025-08-19 18:16:38 [twisted] CRITICAL: Unhandled error in Deferred:
2025-08-19 18:16:38 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 74, in load_object
    obj = getattr(mod, name)
AttributeError: module 'books.pipelines' has no attribute 'BooksPipeline'. Did you mean: 'MongoPipeline'?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\engine.py", line 114, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\scraper.py", line 107, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\utils\misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'books.pipelines' doesn't define any object named 'BooksPipeline'
2025-08-19 18:39:56 [py.warnings] WARNING: C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: books.spiders.book.BookSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-08-19 18:41:31 [py.warnings] WARNING: C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: books.spiders.book.BookSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-08-19 18:43:53 [py.warnings] WARNING: C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: books.spiders.book.BookSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-08-19 18:45:23 [py.warnings] WARNING: C:\Users\Simon\Documents\GitHub\WebDataScraper\WebScraper01\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: books.spiders.book.BookSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

